{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc97999",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ V5: ULTRA-CONSERVATIVE MODEL\n",
    "## Target: Beat 4,000 score (leader)\n",
    "### Current: 13,978 (43rd place) âŒ\n",
    "\n",
    "**Strategy:**\n",
    "1. Lower quantile predictions (use 0.10 instead of 0.2)\n",
    "2. Stronger regularization\n",
    "3. Reduce predictions systematically\n",
    "4. Focus on NOT over-predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad8e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37dfecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receivals: 122,590 rows\n",
      "Purchase Orders: 33,171 rows\n",
      "Prediction mapping: 30,450 rows\n",
      "\n",
      "Date range: 2004-06-15 13:34:00+02:00 to 2024-12-19 13:36:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "receivals = pd.read_csv('data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('data/kernel/purchase_orders.csv')\n",
    "materials = pd.read_csv('data/extended/materials.csv')\n",
    "prediction_mapping = pd.read_csv('data/prediction_mapping.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'])\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'])\n",
    "purchase_orders['created_date_time'] = pd.to_datetime(purchase_orders['created_date_time'])\n",
    "\n",
    "print(f\"Receivals: {len(receivals):,} rows\")\n",
    "print(f\"Purchase Orders: {len(purchase_orders):,} rows\")\n",
    "print(f\"Prediction mapping: {len(prediction_mapping):,} rows\")\n",
    "print(f\"\\nDate range: {receivals['date_arrival'].min()} to {receivals['date_arrival'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4dc94",
   "metadata": {},
   "source": [
    "## ðŸ“Š Analyze Current Predictions vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474e8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical Weight Distributions:\n",
      "Mean of means: 13,376 kg\n",
      "Mean of p10: 7,579 kg\n",
      "Mean of p20: 9,630 kg\n",
      "Mean of p30: 11,025 kg\n",
      "Mean of medians: 13,359 kg\n",
      "\n",
      "Ratio p20/mean: 71.99%\n",
      "Ratio p10/mean: 56.66%\n"
     ]
    }
   ],
   "source": [
    "# What percentile should we ACTUALLY use?\n",
    "# Let's check historical 0.2 quantile vs mean\n",
    "\n",
    "historical_stats = []\n",
    "for rm_id in receivals['rm_id'].unique():\n",
    "    rm_data = receivals[receivals['rm_id'] == rm_id]['net_weight']\n",
    "    if len(rm_data) > 10:\n",
    "        historical_stats.append({\n",
    "            'rm_id': rm_id,\n",
    "            'mean': rm_data.mean(),\n",
    "            'p10': rm_data.quantile(0.10),\n",
    "            'p20': rm_data.quantile(0.20),\n",
    "            'p30': rm_data.quantile(0.30),\n",
    "            'median': rm_data.median(),\n",
    "            'count': len(rm_data)\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(historical_stats)\n",
    "print(\"Historical Weight Distributions:\")\n",
    "print(f\"Mean of means: {stats_df['mean'].mean():,.0f} kg\")\n",
    "print(f\"Mean of p10: {stats_df['p10'].mean():,.0f} kg\")\n",
    "print(f\"Mean of p20: {stats_df['p20'].mean():,.0f} kg\")\n",
    "print(f\"Mean of p30: {stats_df['p30'].mean():,.0f} kg\")\n",
    "print(f\"Mean of medians: {stats_df['median'].mean():,.0f} kg\")\n",
    "print(f\"\\nRatio p20/mean: {stats_df['p20'].mean() / stats_df['mean'].mean():.2%}\")\n",
    "print(f\"Ratio p10/mean: {stats_df['p10'].mean() / stats_df['mean'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76227596",
   "metadata": {},
   "source": [
    "## ðŸ”§ Enhanced Feature Engineering - ULTRA CONSERVATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3cd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ultra_conservative_features(receivals, purchase_orders, materials, rm_id, current_date, forecast_horizon_days):\n",
    "    \"\"\"\n",
    "    Calculate features with EXTREME conservative bias\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Historical data up to current_date\n",
    "    hist = receivals[(receivals['rm_id'] == rm_id) & (receivals['date_arrival'] <= current_date)]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        # No history = predict ZERO or very small\n",
    "        return {\n",
    "            'rm_id_encoded': rm_id,\n",
    "            'forecast_horizon_days': forecast_horizon_days,\n",
    "            'has_history': 0,\n",
    "            **{f'feature_{i}': 0 for i in range(25)}\n",
    "        }\n",
    "    \n",
    "    # Basic info\n",
    "    features['rm_id_encoded'] = rm_id\n",
    "    features['forecast_horizon_days'] = forecast_horizon_days\n",
    "    features['has_history'] = 1\n",
    "    \n",
    "    # Time-based features\n",
    "    target_date = current_date + pd.Timedelta(days=forecast_horizon_days)\n",
    "    features['month'] = target_date.month\n",
    "    features['quarter'] = target_date.quarter\n",
    "    features['day_of_year'] = target_date.dayofyear\n",
    "    \n",
    "    # Seasonality (sin/cos)\n",
    "    features['month_sin'] = np.sin(2 * np.pi * target_date.month / 12)\n",
    "    features['month_cos'] = np.cos(2 * np.pi * target_date.month / 12)\n",
    "    \n",
    "    # Historical statistics - USE LOWER QUANTILES!\n",
    "    features['hist_p05'] = hist['net_weight'].quantile(0.05)  # 5th percentile!\n",
    "    features['hist_p10'] = hist['net_weight'].quantile(0.10)  # 10th percentile\n",
    "    features['hist_p15'] = hist['net_weight'].quantile(0.15)  # 15th percentile\n",
    "    features['hist_p20'] = hist['net_weight'].quantile(0.20)\n",
    "    features['hist_mean'] = hist['net_weight'].mean()\n",
    "    features['hist_std'] = hist['net_weight'].std()\n",
    "    features['hist_cv'] = features['hist_std'] / (features['hist_mean'] + 1)\n",
    "    \n",
    "    # Recent trends (last 30, 60, 90 days) - weighted toward LOWER values\n",
    "    for days in [30, 60, 90]:\n",
    "        recent = hist[hist['date_arrival'] >= (current_date - pd.Timedelta(days=days))]\n",
    "        if len(recent) > 0:\n",
    "            features[f'recent_{days}d_p10'] = recent['net_weight'].quantile(0.10)\n",
    "            features[f'recent_{days}d_count'] = len(recent)\n",
    "            features[f'recent_{days}d_daily_rate'] = recent['net_weight'].sum() / days\n",
    "        else:\n",
    "            features[f'recent_{days}d_p10'] = 0\n",
    "            features[f'recent_{days}d_count'] = 0\n",
    "            features[f'recent_{days}d_daily_rate'] = 0\n",
    "    \n",
    "    # Delivery frequency\n",
    "    if len(hist) > 1:\n",
    "        date_range = (hist['date_arrival'].max() - hist['date_arrival'].min()).days\n",
    "        features['delivery_frequency'] = len(hist) / max(date_range, 1)\n",
    "        features['avg_days_between'] = date_range / max(len(hist) - 1, 1)\n",
    "    else:\n",
    "        features['delivery_frequency'] = 0\n",
    "        features['avg_days_between'] = 999\n",
    "    \n",
    "    # Days since last delivery\n",
    "    features['days_since_last'] = (current_date - hist['date_arrival'].max()).days\n",
    "    \n",
    "    # Purchase orders - CONSERVATIVE interpretation\n",
    "    # Map product_id to rm_id using materials dataset\n",
    "    rm_products = materials[materials['rm_id'] == rm_id]['product_id'].dropna().unique()\n",
    "    \n",
    "    if len(rm_products) > 0:\n",
    "        future_orders = purchase_orders[\n",
    "            (purchase_orders['product_id'].isin(rm_products)) &\n",
    "            (purchase_orders['delivery_date'] > current_date) &\n",
    "            (purchase_orders['delivery_date'] <= target_date)\n",
    "        ]\n",
    "        \n",
    "        features['future_orders_count'] = len(future_orders)\n",
    "        features['future_orders_weight'] = future_orders['quantity'].sum() if len(future_orders) > 0 else 0\n",
    "        # DISCOUNT future orders by 30% (uncertainty)\n",
    "        features['future_orders_weight_discounted'] = features['future_orders_weight'] * 0.7\n",
    "    else:\n",
    "        features['future_orders_count'] = 0\n",
    "        features['future_orders_weight'] = 0\n",
    "        features['future_orders_weight_discounted'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bebea0",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 15000 training samples from 103 materials...\n",
      "âœ… Created 15000 samples\n",
      "Target stats: mean=614,558, median=145,947\n",
      "Target p20=22,300, p10=1,080\n"
     ]
    }
   ],
   "source": [
    "def create_ultra_conservative_training_samples(receivals, purchase_orders, materials, n_samples=15000):\n",
    "    \"\"\"\n",
    "    Create training samples focusing on UNDER-prediction\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    samples = []\n",
    "    \n",
    "    # Get materials with sufficient history\n",
    "    material_counts = receivals['rm_id'].value_counts()\n",
    "    valid_materials = material_counts[material_counts >= 20].index.tolist()\n",
    "    \n",
    "    print(f\"Creating {n_samples} training samples from {len(valid_materials)} materials...\")\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random material\n",
    "        rm_id = np.random.choice(valid_materials)\n",
    "        rm_data = receivals[receivals['rm_id'] == rm_id].sort_values('date_arrival')\n",
    "        \n",
    "        if len(rm_data) < 20:\n",
    "            continue\n",
    "        \n",
    "        # Random split point (use 80% of data for history)\n",
    "        split_idx = np.random.randint(int(len(rm_data) * 0.5), int(len(rm_data) * 0.95))\n",
    "        current_date = rm_data.iloc[split_idx]['date_arrival']\n",
    "        \n",
    "        # Random forecast horizon (1 to 150 days)\n",
    "        horizon_days = np.random.randint(1, 151)\n",
    "        target_date = current_date + pd.Timedelta(days=horizon_days)\n",
    "        \n",
    "        # Calculate features\n",
    "        features = calculate_ultra_conservative_features(\n",
    "            rm_data.iloc[:split_idx],\n",
    "            purchase_orders,\n",
    "            materials,\n",
    "            rm_id,\n",
    "            current_date,\n",
    "            horizon_days\n",
    "        )\n",
    "        \n",
    "        # Calculate actual cumulative weight\n",
    "        actual = rm_data[\n",
    "            (rm_data['date_arrival'] > current_date) &\n",
    "            (rm_data['date_arrival'] <= target_date)\n",
    "        ]['net_weight'].sum()\n",
    "        \n",
    "        features['target'] = actual\n",
    "        samples.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    print(f\"âœ… Created {len(df)} samples\")\n",
    "    print(f\"Target stats: mean={df['target'].mean():,.0f}, median={df['target'].median():,.0f}\")\n",
    "    print(f\"Target p20={df['target'].quantile(0.20):,.0f}, p10={df['target'].quantile(0.10):,.0f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_ultra_conservative_training_samples(receivals, purchase_orders, materials, n_samples=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f4a66",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Train ULTRA-CONSERVATIVE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ca75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11,250\n",
      "Validation samples: 3,750\n",
      "\n",
      "ðŸš€ Training ULTRA-CONSERVATIVE model (quantile 0.10)...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's quantile: 56280.9\n",
      "[100]\tvalid_0's quantile: 56280.9\n",
      "[200]\tvalid_0's quantile: 52893.1\n",
      "[200]\tvalid_0's quantile: 52893.1\n",
      "[300]\tvalid_0's quantile: 49850.3\n",
      "[400]\tvalid_0's quantile: 47644.5\n",
      "[300]\tvalid_0's quantile: 49850.3\n",
      "[400]\tvalid_0's quantile: 47644.5\n",
      "[500]\tvalid_0's quantile: 45629.3\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's quantile: 45629.3\n",
      "\n",
      "âœ… Model trained!\n",
      "[500]\tvalid_0's quantile: 45629.3\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's quantile: 45629.3\n",
      "\n",
      "âœ… Model trained!\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "feature_cols = [col for col in train_df.columns if col != 'target']\n",
    "X = train_df[feature_cols].fillna(0)\n",
    "y = train_df['target']\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "\n",
    "# ULTRA-CONSERVATIVE LightGBM params - TARGET QUANTILE 0.10!\n",
    "params_ultra = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.10,  # 10th percentile instead of 20th!\n",
    "    'metric': 'quantile',\n",
    "    'num_leaves': 20,  # Very simple trees\n",
    "    'learning_rate': 0.01,  # Slower learning\n",
    "    'feature_fraction': 0.6,  # Use fewer features\n",
    "    'bagging_fraction': 0.6,  # Use fewer samples\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 50,  # Stronger regularization\n",
    "    'lambda_l1': 2.0,  # Strong L1\n",
    "    'lambda_l2': 2.0,  # Strong L2\n",
    "    'max_depth': 5,  # Shallow trees\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "print(\"\\nðŸš€ Training ULTRA-CONSERVATIVE model (quantile 0.10)...\")\n",
    "model_ultra = lgb.train(\n",
    "    params_ultra,\n",
    "    lgb_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgb_val],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034df41",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2fe1356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ VALIDATION RESULTS (ULTRA-CONSERVATIVE):\n",
      "\n",
      "Quantile Loss (0.2): 332,609,607\n",
      "\n",
      "Predictions:\n",
      "  Mean: 162,894 kg\n",
      "  Median: 62,522 kg\n",
      "  Zero predictions: 323 (8.6%)\n",
      "\n",
      "Actuals:\n",
      "  Mean: 593,634 kg\n",
      "  Median: 140,422 kg\n",
      "\n",
      "Error distribution:\n",
      "  Over-predictions: 403 (10.7%)\n",
      "  Under-predictions: 3134 (83.6%)\n",
      "\n",
      "Target: ~80% under-predictions for quantile 0.2\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation\n",
    "preds_val_ultra = model_ultra.predict(X_val)\n",
    "preds_val_ultra = np.maximum(preds_val_ultra, 0)  # No negative\n",
    "\n",
    "# Calculate quantile loss at 0.2 (competition metric)\n",
    "def quantile_loss(y_true, y_pred, quantile=0.2):\n",
    "    errors = y_true - y_pred\n",
    "    loss = np.where(errors >= 0, quantile * errors, (quantile - 1) * errors)\n",
    "    return loss.sum()\n",
    "\n",
    "ql_ultra = quantile_loss(y_val.values, preds_val_ultra, quantile=0.2)\n",
    "\n",
    "# Analysis\n",
    "over_preds = np.sum(preds_val_ultra > y_val.values)\n",
    "under_preds = np.sum(preds_val_ultra < y_val.values)\n",
    "\n",
    "print(f\"ðŸŽ¯ VALIDATION RESULTS (ULTRA-CONSERVATIVE):\")\n",
    "print(f\"\\nQuantile Loss (0.2): {ql_ultra:,.0f}\")\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  Mean: {preds_val_ultra.mean():,.0f} kg\")\n",
    "print(f\"  Median: {np.median(preds_val_ultra):,.0f} kg\")\n",
    "print(f\"  Zero predictions: {np.sum(preds_val_ultra == 0)} ({np.sum(preds_val_ultra == 0)/len(preds_val_ultra):.1%})\")\n",
    "print(f\"\\nActuals:\")\n",
    "print(f\"  Mean: {y_val.mean():,.0f} kg\")\n",
    "print(f\"  Median: {y_val.median():,.0f} kg\")\n",
    "print(f\"\\nError distribution:\")\n",
    "print(f\"  Over-predictions: {over_preds} ({over_preds/len(y_val):.1%})\")\n",
    "print(f\"  Under-predictions: {under_preds} ({under_preds/len(y_val):.1%})\")\n",
    "print(f\"\\nTarget: ~80% under-predictions for quantile 0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea41015",
   "metadata": {},
   "source": [
    "## ðŸ”® Generate Competition Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5718182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ULTRA-CONSERVATIVE predictions...\n",
      "  Processed 5,000/30,450\n",
      "  Processed 5,000/30,450\n",
      "  Processed 10,000/30,450\n",
      "  Processed 10,000/30,450\n",
      "  Processed 15,000/30,450\n",
      "  Processed 15,000/30,450\n",
      "  Processed 20,000/30,450\n",
      "  Processed 20,000/30,450\n",
      "  Processed 25,000/30,450\n",
      "  Processed 25,000/30,450\n",
      "  Processed 30,000/30,450\n",
      "  Processed 30,000/30,450\n",
      "\n",
      "âœ… Predictions generated!\n",
      "\n",
      "Statistics:\n",
      "  Mean: 19,546 kg\n",
      "  Median: 637 kg\n",
      "  Min: 0 kg\n",
      "  Max: 1,123,722 kg\n",
      "  Zero predictions: 9146 (30.0%)\n",
      "\n",
      "Percentiles:\n",
      "  25th: 0 kg\n",
      "  50th: 637 kg\n",
      "  75th: 4,049 kg\n",
      "\n",
      "âœ… Predictions generated!\n",
      "\n",
      "Statistics:\n",
      "  Mean: 19,546 kg\n",
      "  Median: 637 kg\n",
      "  Min: 0 kg\n",
      "  Max: 1,123,722 kg\n",
      "  Zero predictions: 9146 (30.0%)\n",
      "\n",
      "Percentiles:\n",
      "  25th: 0 kg\n",
      "  50th: 637 kg\n",
      "  75th: 4,049 kg\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for competition\n",
    "pred_start_date = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "prediction_mapping['horizon_days'] = (prediction_mapping['forecast_end_date'] - pd.to_datetime('2025-01-01')).dt.days\n",
    "\n",
    "predictions_ultra = []\n",
    "\n",
    "print(\"Generating ULTRA-CONSERVATIVE predictions...\")\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    horizon_days = row['horizon_days']\n",
    "    \n",
    "    # Calculate features\n",
    "    features = calculate_ultra_conservative_features(\n",
    "        receivals,\n",
    "        purchase_orders,\n",
    "        materials,\n",
    "        rm_id,\n",
    "        pred_start_date,\n",
    "        horizon_days\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_pred = pd.DataFrame([features])[feature_cols].fillna(0)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model_ultra.predict(X_pred)[0]\n",
    "    pred = max(0, pred)  # No negative\n",
    "    \n",
    "    predictions_ultra.append(pred)\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"  Processed {idx + 1:,}/{len(prediction_mapping):,}\")\n",
    "\n",
    "predictions_ultra = np.array(predictions_ultra)\n",
    "\n",
    "print(f\"\\nâœ… Predictions generated!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean: {predictions_ultra.mean():,.0f} kg\")\n",
    "print(f\"  Median: {np.median(predictions_ultra):,.0f} kg\")\n",
    "print(f\"  Min: {predictions_ultra.min():,.0f} kg\")\n",
    "print(f\"  Max: {predictions_ultra.max():,.0f} kg\")\n",
    "print(f\"  Zero predictions: {np.sum(predictions_ultra == 0)} ({np.sum(predictions_ultra == 0)/len(predictions_ultra):.1%})\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "print(f\"  25th: {np.percentile(predictions_ultra, 25):,.0f} kg\")\n",
    "print(f\"  50th: {np.percentile(predictions_ultra, 50):,.0f} kg\")\n",
    "print(f\"  75th: {np.percentile(predictions_ultra, 75):,.0f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53307824",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a02192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission saved as 'submission_v5_ultra_conservative.csv'\n",
      "\n",
      "First 20 predictions:\n",
      "    ID  predicted_weight\n",
      "0    1               0.0\n",
      "1    2               0.0\n",
      "2    3               0.0\n",
      "3    4               0.0\n",
      "4    5               0.0\n",
      "5    6               0.0\n",
      "6    7               0.0\n",
      "7    8               0.0\n",
      "8    9               0.0\n",
      "9   10               0.0\n",
      "10  11               0.0\n",
      "11  12               0.0\n",
      "12  13               0.0\n",
      "13  14               0.0\n",
      "14  15               0.0\n",
      "15  16               0.0\n",
      "16  17               0.0\n",
      "17  18               0.0\n",
      "18  19               0.0\n",
      "19  20               0.0\n",
      "\n",
      "============================================================\n",
      "FINAL COMPARISON\n",
      "============================================================\n",
      "\n",
      "V4 (tuned, quantile 0.2): mean ~67,000 kg â†’ Score: 13,978 (43rd)\n",
      "V5 (ultra-conservative, quantile 0.10): mean ~19,546 kg â†’ Score: ???\n",
      "\n",
      "Target: Beat leader score of 4,000\n",
      "Strategy: Much more conservative predictions (quantile 0.10 instead of 0.20)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "submission_ultra = pd.DataFrame({\n",
    "    'ID': range(1, len(predictions_ultra) + 1),\n",
    "    'predicted_weight': predictions_ultra\n",
    "})\n",
    "\n",
    "submission_ultra.to_csv('submission_v5_ultra_conservative.csv', index=False)\n",
    "print(\"âœ… Submission saved as 'submission_v5_ultra_conservative.csv'\")\n",
    "\n",
    "print(\"\\nFirst 20 predictions:\")\n",
    "print(submission_ultra.head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nV4 (tuned, quantile 0.2): mean ~67,000 kg â†’ Score: 13,978 (43rd)\")\n",
    "print(f\"V5 (ultra-conservative, quantile 0.10): mean ~{predictions_ultra.mean():,.0f} kg â†’ Score: ???\")\n",
    "print(f\"\\nTarget: Beat leader score of 4,000\")\n",
    "print(f\"Strategy: Much more conservative predictions (quantile 0.10 instead of 0.20)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e312ee",
   "metadata": {},
   "source": [
    "## ðŸ”¥ V6 EXTREME: Scale down predictions by 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e36793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… V6 EXTREME submission saved as 'submission_v6_extreme.csv'\n",
      "\n",
      "ðŸ“Š V6 STATISTICS:\n",
      "  Mean: 9,773 kg (50% of V5)\n",
      "  Median: 319 kg\n",
      "  Zero predictions: 9146 (30.0%)\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATION:\n",
      "======================================================================\n",
      "Try these in order:\n",
      "  1. submission_v5_ultra_conservative.csv (mean 19,546 kg)\n",
      "  2. submission_v6_extreme.csv (mean ~9,773 kg) if V5 still too high\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# V6: Apply 50% scaling to be EVEN MORE conservative\n",
    "predictions_v6 = predictions_ultra * 0.5\n",
    "\n",
    "submission_v6 = pd.DataFrame({\n",
    "    'ID': range(1, len(predictions_v6) + 1),\n",
    "    'predicted_weight': predictions_v6\n",
    "})\n",
    "\n",
    "submission_v6.to_csv('submission_v6_extreme.csv', index=False)\n",
    "print(\"âœ… V6 EXTREME submission saved as 'submission_v6_extreme.csv'\")\n",
    "\n",
    "print(f\"\\nðŸ“Š V6 STATISTICS:\")\n",
    "print(f\"  Mean: {predictions_v6.mean():,.0f} kg (50% of V5)\")\n",
    "print(f\"  Median: {np.median(predictions_v6):,.0f} kg\")\n",
    "print(f\"  Zero predictions: {np.sum(predictions_v6 == 0)} ({np.sum(predictions_v6 == 0)/len(predictions_v6):.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Try these in order:\")\n",
    "print(\"  1. submission_v5_ultra_conservative.csv (mean 19,546 kg)\")\n",
    "print(\"  2. submission_v6_extreme.csv (mean ~9,773 kg) if V5 still too high\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d8a10",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ V7 ULTIMATE: Train with quantile 0.05 (5th percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c49144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training V7 ULTIMATE model (quantile 0.05)...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's quantile: 28787.3\n",
      "[200]\tvalid_0's quantile: 28060.7\n",
      "[300]\tvalid_0's quantile: 27200.4\n",
      "[200]\tvalid_0's quantile: 28060.7\n",
      "[300]\tvalid_0's quantile: 27200.4\n",
      "[400]\tvalid_0's quantile: 26458.3\n",
      "[500]\tvalid_0's quantile: 26003.7\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's quantile: 26003.7\n",
      "\n",
      "ðŸŽ¯ V7 VALIDATION (quantile 0.05):\n",
      "  Quantile Loss (0.2): 380,722,870\n",
      "  Mean prediction: 90,109 kg\n",
      "  Under-predictions: 3280 (87.5%)\n",
      "[400]\tvalid_0's quantile: 26458.3\n",
      "[500]\tvalid_0's quantile: 26003.7\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's quantile: 26003.7\n",
      "\n",
      "ðŸŽ¯ V7 VALIDATION (quantile 0.05):\n",
      "  Quantile Loss (0.2): 380,722,870\n",
      "  Mean prediction: 90,109 kg\n",
      "  Under-predictions: 3280 (87.5%)\n"
     ]
    }
   ],
   "source": [
    "# Train with QUANTILE 0.05\n",
    "params_ultimate = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.05,  # 5th percentile!\n",
    "    'metric': 'quantile',\n",
    "    'num_leaves': 15,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'lambda_l1': 3.0,\n",
    "    'lambda_l2': 3.0,\n",
    "    'max_depth': 4,\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Training V7 ULTIMATE model (quantile 0.05)...\")\n",
    "model_v7 = lgb.train(\n",
    "    params_ultimate,\n",
    "    lgb_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgb_val],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict on validation\n",
    "preds_v7_val = model_v7.predict(X_val)\n",
    "preds_v7_val = np.maximum(preds_v7_val, 0)\n",
    "\n",
    "ql_v7 = quantile_loss(y_val.values, preds_v7_val, quantile=0.2)\n",
    "over_v7 = np.sum(preds_v7_val > y_val.values)\n",
    "under_v7 = np.sum(preds_v7_val < y_val.values)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ V7 VALIDATION (quantile 0.05):\") \n",
    "print(f\"  Quantile Loss (0.2): {ql_v7:,.0f}\")\n",
    "print(f\"  Mean prediction: {preds_v7_val.mean():,.0f} kg\")\n",
    "print(f\"  Under-predictions: {under_v7} ({under_v7/len(y_val):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69121455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate V7 predictions\n",
    "predictions_v7 = []\n",
    "\n",
    "print(\"Generating V7 predictions (quantile 0.05)...\")\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    horizon_days = row['horizon_days']\n",
    "    \n",
    "    features = calculate_ultra_conservative_features(\n",
    "        receivals, purchase_orders, materials,\n",
    "        rm_id, pred_start_date, horizon_days\n",
    "    )\n",
    "    \n",
    "    X_pred = pd.DataFrame([features])[feature_cols].fillna(0)\n",
    "    pred = model_v7.predict(X_pred)[0]\n",
    "    predictions_v7.append(max(0, pred))\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"  {idx + 1:,}/30,450\")\n",
    "\n",
    "predictions_v7 = np.array(predictions_v7)\n",
    "\n",
    "submission_v7 = pd.DataFrame({\n",
    "    'ID': range(1, len(predictions_v7) + 1),\n",
    "    'predicted_weight': predictions_v7\n",
    "})\n",
    "\n",
    "submission_v7.to_csv('submission_v7_ultimate.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… V7 saved as 'submission_v7_ultimate.csv'\")\n",
    "print(f\"  Mean: {predictions_v7.mean():,.0f} kg\")\n",
    "print(f\"  Median: {np.median(predictions_v7):,.0f} kg\")\n",
    "print(f\"  Zeros: {np.sum(predictions_v7 == 0)} ({np.sum(predictions_v7 == 0)/len(predictions_v7):.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
