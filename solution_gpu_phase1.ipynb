{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464c4b63",
   "metadata": {},
   "source": [
    "# üöÄ GPU-ACCELERATED MODEL - FASE 1: QUICK WINS\n",
    "## Target: Migliorare da 13,978 ‚Üí ~9,000-10,000 score\n",
    "\n",
    "**Strategie:**\n",
    "1. CatBoost GPU con 10,000 iterazioni\n",
    "2. XGBoost GPU ottimizzato\n",
    "3. Feature Engineering Avanzato (100+ features)\n",
    "4. Optuna con 1000+ trials\n",
    "5. Ensemble pesante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU libraries\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    print(\"‚úÖ CatBoost available\")\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå CatBoost not installed. Run: pip install catboost\")\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.integration import LightGBMPruningCallback\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå Optuna not installed. Run: pip install optuna\")\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "print(\"\\n‚úÖ Core libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164403c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "receivals = pd.read_csv('data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv('data/kernel/purchase_orders.csv')\n",
    "materials = pd.read_csv('data/extended/materials.csv')\n",
    "prediction_mapping = pd.read_csv('data/prediction_mapping.csv')\n",
    "\n",
    "# Convert dates\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'])\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'])\n",
    "\n",
    "print(f\"Receivals: {len(receivals):,} rows\")\n",
    "print(f\"Purchase Orders: {len(purchase_orders):,} rows\")\n",
    "print(f\"Prediction mapping: {len(prediction_mapping):,} rows\")\n",
    "print(f\"\\nDate range: {receivals['date_arrival'].min()} to {receivals['date_arrival'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1028b89",
   "metadata": {},
   "source": [
    "## üîß ADVANCED FEATURE ENGINEERING (100+ Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead25cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_features_v2(receivals, purchase_orders, materials, rm_id, current_date, forecast_horizon_days):\n",
    "    \"\"\"\n",
    "    Calculate 100+ advanced features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Historical data\n",
    "    hist = receivals[(receivals['rm_id'] == rm_id) & (receivals['date_arrival'] <= current_date)]\n",
    "    \n",
    "    if len(hist) == 0:\n",
    "        return {'rm_id_encoded': rm_id, 'forecast_horizon_days': forecast_horizon_days, 'has_history': 0,\n",
    "                **{f'feature_{i}': 0 for i in range(100)}}\n",
    "    \n",
    "    # Basic\n",
    "    features['rm_id_encoded'] = rm_id\n",
    "    features['forecast_horizon_days'] = forecast_horizon_days\n",
    "    features['has_history'] = 1\n",
    "    features['total_deliveries'] = len(hist)\n",
    "    \n",
    "    # Target date features\n",
    "    target_date = current_date + pd.Timedelta(days=forecast_horizon_days)\n",
    "    features['month'] = target_date.month\n",
    "    features['quarter'] = target_date.quarter\n",
    "    features['day_of_year'] = target_date.dayofyear\n",
    "    features['week_of_year'] = target_date.isocalendar()[1]\n",
    "    features['is_quarter_end'] = int(target_date.month in [3, 6, 9, 12])\n",
    "    features['is_year_end'] = int(target_date.month == 12)\n",
    "    \n",
    "    # Multiple seasonality encodings\n",
    "    features['month_sin'] = np.sin(2 * np.pi * target_date.month / 12)\n",
    "    features['month_cos'] = np.cos(2 * np.pi * target_date.month / 12)\n",
    "    features['quarter_sin'] = np.sin(2 * np.pi * target_date.quarter / 4)\n",
    "    features['quarter_cos'] = np.cos(2 * np.pi * target_date.quarter / 4)\n",
    "    features['week_sin'] = np.sin(2 * np.pi * target_date.isocalendar()[1] / 52)\n",
    "    features['week_cos'] = np.cos(2 * np.pi * target_date.isocalendar()[1] / 52)\n",
    "    \n",
    "    # Historical statistics - MULTIPLE QUANTILES\n",
    "    for q in [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.50, 0.75, 0.90]:\n",
    "        features[f'hist_p{int(q*100)}'] = hist['net_weight'].quantile(q)\n",
    "    \n",
    "    features['hist_mean'] = hist['net_weight'].mean()\n",
    "    features['hist_std'] = hist['net_weight'].std()\n",
    "    features['hist_cv'] = features['hist_std'] / (features['hist_mean'] + 1)\n",
    "    features['hist_skew'] = hist['net_weight'].skew()\n",
    "    features['hist_kurt'] = hist['net_weight'].kurtosis()\n",
    "    \n",
    "    # ROLLING WINDOWS - Multiple periods\n",
    "    for window in [7, 14, 30, 60, 90, 180, 365]:\n",
    "        recent = hist[hist['date_arrival'] >= (current_date - pd.Timedelta(days=window))]\n",
    "        \n",
    "        if len(recent) > 0:\n",
    "            features[f'roll_{window}d_mean'] = recent['net_weight'].mean()\n",
    "            features[f'roll_{window}d_std'] = recent['net_weight'].std()\n",
    "            features[f'roll_{window}d_p10'] = recent['net_weight'].quantile(0.10)\n",
    "            features[f'roll_{window}d_p20'] = recent['net_weight'].quantile(0.20)\n",
    "            features[f'roll_{window}d_p50'] = recent['net_weight'].quantile(0.50)\n",
    "            features[f'roll_{window}d_count'] = len(recent)\n",
    "            features[f'roll_{window}d_sum'] = recent['net_weight'].sum()\n",
    "            features[f'roll_{window}d_daily_rate'] = recent['net_weight'].sum() / window\n",
    "            \n",
    "            # Trend (linear regression)\n",
    "            if len(recent) > 2:\n",
    "                x = np.arange(len(recent))\n",
    "                y = recent['net_weight'].values\n",
    "                slope = np.polyfit(x, y, 1)[0] if len(x) > 0 else 0\n",
    "                features[f'roll_{window}d_trend'] = slope\n",
    "            else:\n",
    "                features[f'roll_{window}d_trend'] = 0\n",
    "        else:\n",
    "            features[f'roll_{window}d_mean'] = 0\n",
    "            features[f'roll_{window}d_std'] = 0\n",
    "            features[f'roll_{window}d_p10'] = 0\n",
    "            features[f'roll_{window}d_p20'] = 0\n",
    "            features[f'roll_{window}d_p50'] = 0\n",
    "            features[f'roll_{window}d_count'] = 0\n",
    "            features[f'roll_{window}d_sum'] = 0\n",
    "            features[f'roll_{window}d_daily_rate'] = 0\n",
    "            features[f'roll_{window}d_trend'] = 0\n",
    "    \n",
    "    # LAG FEATURES\n",
    "    hist_sorted = hist.sort_values('date_arrival', ascending=False)\n",
    "    for lag in [1, 7, 14, 30, 60, 90]:\n",
    "        lag_data = hist_sorted[hist_sorted['date_arrival'] <= (current_date - pd.Timedelta(days=lag))]\n",
    "        if len(lag_data) > 0:\n",
    "            features[f'lag_{lag}d_weight'] = lag_data.iloc[0]['net_weight']\n",
    "        else:\n",
    "            features[f'lag_{lag}d_weight'] = 0\n",
    "    \n",
    "    # Delivery frequency\n",
    "    if len(hist) > 1:\n",
    "        date_range = (hist['date_arrival'].max() - hist['date_arrival'].min()).days\n",
    "        features['delivery_frequency'] = len(hist) / max(date_range, 1)\n",
    "        features['avg_days_between'] = date_range / max(len(hist) - 1, 1)\n",
    "    else:\n",
    "        features['delivery_frequency'] = 0\n",
    "        features['avg_days_between'] = 999\n",
    "    \n",
    "    # Days since last delivery\n",
    "    features['days_since_last'] = (current_date - hist['date_arrival'].max()).days\n",
    "    features['recency_score'] = 1 / (1 + features['days_since_last'] / 30)  # Decay factor\n",
    "    \n",
    "    # Purchase orders\n",
    "    rm_products = materials[materials['rm_id'] == rm_id]['product_id'].dropna().unique()\n",
    "    \n",
    "    if len(rm_products) > 0:\n",
    "        future_orders = purchase_orders[\n",
    "            (purchase_orders['product_id'].isin(rm_products)) &\n",
    "            (purchase_orders['delivery_date'] > current_date) &\n",
    "            (purchase_orders['delivery_date'] <= target_date)\n",
    "        ]\n",
    "        \n",
    "        features['future_orders_count'] = len(future_orders)\n",
    "        features['future_orders_qty'] = future_orders['quantity'].sum() if len(future_orders) > 0 else 0\n",
    "        features['future_orders_avg'] = future_orders['quantity'].mean() if len(future_orders) > 0 else 0\n",
    "    else:\n",
    "        features['future_orders_count'] = 0\n",
    "        features['future_orders_qty'] = 0\n",
    "        features['future_orders_avg'] = 0\n",
    "    \n",
    "    # INTERACTION FEATURES\n",
    "    features['horizon_x_freq'] = forecast_horizon_days * features['delivery_frequency']\n",
    "    features['horizon_x_recency'] = forecast_horizon_days * features['recency_score']\n",
    "    features['horizon_x_trend_30'] = forecast_horizon_days * features['roll_30d_trend']\n",
    "    features['month_x_p20'] = features['month'] * features['hist_p20']\n",
    "    \n",
    "    # MOMENTUM features\n",
    "    features['momentum_30_60'] = features['roll_30d_mean'] - features['roll_60d_mean']\n",
    "    features['momentum_60_90'] = features['roll_60d_mean'] - features['roll_90d_mean']\n",
    "    features['acceleration'] = features['momentum_30_60'] - features['momentum_60_90']\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Advanced feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7dc4c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Create Training Dataset (50,000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpu_training_samples(receivals, purchase_orders, materials, n_samples=50000):\n",
    "    \"\"\"\n",
    "    Create massive training dataset for GPU training\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    samples = []\n",
    "    \n",
    "    material_counts = receivals['rm_id'].value_counts()\n",
    "    valid_materials = material_counts[material_counts >= 20].index.tolist()\n",
    "    \n",
    "    print(f\"Creating {n_samples:,} training samples from {len(valid_materials)} materials...\")\n",
    "    print(\"This will take ~5-10 minutes...\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"  {i + 1:,}/{n_samples:,} samples...\")\n",
    "        \n",
    "        rm_id = np.random.choice(valid_materials)\n",
    "        rm_data = receivals[receivals['rm_id'] == rm_id].sort_values('date_arrival')\n",
    "        \n",
    "        if len(rm_data) < 20:\n",
    "            continue\n",
    "        \n",
    "        split_idx = np.random.randint(int(len(rm_data) * 0.5), int(len(rm_data) * 0.95))\n",
    "        current_date = rm_data.iloc[split_idx]['date_arrival']\n",
    "        horizon_days = np.random.randint(1, 151)\n",
    "        target_date = current_date + pd.Timedelta(days=horizon_days)\n",
    "        \n",
    "        features = calculate_advanced_features_v2(\n",
    "            rm_data.iloc[:split_idx],\n",
    "            purchase_orders,\n",
    "            materials,\n",
    "            rm_id,\n",
    "            current_date,\n",
    "            horizon_days\n",
    "        )\n",
    "        \n",
    "        actual = rm_data[\n",
    "            (rm_data['date_arrival'] > current_date) &\n",
    "            (rm_data['date_arrival'] <= target_date)\n",
    "        ]['net_weight'].sum()\n",
    "        \n",
    "        features['target'] = actual\n",
    "        samples.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    print(f\"\\n‚úÖ Created {len(df):,} samples\")\n",
    "    print(f\"Features: {len([c for c in df.columns if c != 'target'])}\")\n",
    "    print(f\"Target stats: mean={df['target'].mean():,.0f}, p20={df['target'].quantile(0.20):,.0f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# NOTE: Start with 15K samples for testing, then increase to 50K for final training\n",
    "train_df_gpu = create_gpu_training_samples(receivals, purchase_orders, materials, n_samples=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701e9f2",
   "metadata": {},
   "source": [
    "## üöÄ CATBOOST GPU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71157168",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CATBOOST_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è CatBoost not available. Skipping...\")\n",
    "else:\n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in train_df_gpu.columns if col != 'target']\n",
    "    X = train_df_gpu[feature_cols].fillna(0)\n",
    "    y = train_df_gpu['target']\n",
    "    \n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train):,}\")\n",
    "    print(f\"Validation samples: {len(X_val):,}\")\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # CatBoost GPU params - AGGRESSIVE!\n",
    "    params_catboost = {\n",
    "        'loss_function': 'Quantile:alpha=0.2',\n",
    "        'eval_metric': 'Quantile:alpha=0.2',\n",
    "        'task_type': 'GPU',  # Use GPU!\n",
    "        'devices': '0',\n",
    "        'iterations': 5000,  # Much more iterations\n",
    "        'depth': 8,\n",
    "        'learning_rate': 0.03,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'bootstrap_type': 'Bayesian',\n",
    "        'random_strength': 1,\n",
    "        'bagging_temperature': 1,\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 100,\n",
    "        'random_seed': 42,\n",
    "        'verbose': 100\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüöÄ Training CatBoost GPU model...\")\n",
    "    print(\"This will take 5-15 minutes depending on GPU...\\n\")\n",
    "    \n",
    "    model_catboost = CatBoostRegressor(**params_catboost)\n",
    "    model_catboost.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        use_best_model=True,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ CatBoost model trained!\")\n",
    "    print(f\"Best iteration: {model_catboost.get_best_iteration()}\")\n",
    "    print(f\"Best score: {model_catboost.get_best_score()['validation']['Quantile:alpha=0.2']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e766211",
   "metadata": {},
   "source": [
    "## ‚ö° XGBOOST GPU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost GPU\n",
    "params_xgb_gpu = {\n",
    "    'objective': 'reg:quantileerror',\n",
    "    'quantile_alpha': 0.2,\n",
    "    'tree_method': 'hist',  # Use 'gpu_hist' if GPU available\n",
    "    'device': 'cuda',  # Use GPU\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 2,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training XGBoost GPU model...\\n\")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "model_xgb = xgb.train(\n",
    "    params_xgb_gpu,\n",
    "    dtrain,\n",
    "    num_boost_round=3000,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ XGBoost GPU model trained!\")\n",
    "print(f\"Best iteration: {model_xgb.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe92aa",
   "metadata": {},
   "source": [
    "## üìä Validation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313324e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile=0.2):\n",
    "    errors = y_true - y_pred\n",
    "    loss = np.where(errors >= 0, quantile * errors, (quantile - 1) * errors)\n",
    "    return loss.sum()\n",
    "\n",
    "# Predictions\n",
    "if CATBOOST_AVAILABLE:\n",
    "    preds_catboost = model_catboost.predict(X_val)\n",
    "    preds_catboost = np.maximum(preds_catboost, 0)\n",
    "    ql_catboost = quantile_loss(y_val.values, preds_catboost)\n",
    "\n",
    "preds_xgb = model_xgb.predict(dval)\n",
    "preds_xgb = np.maximum(preds_xgb, 0)\n",
    "ql_xgb = quantile_loss(y_val.values, preds_xgb)\n",
    "\n",
    "# Ensemble (simple average)\n",
    "if CATBOOST_AVAILABLE:\n",
    "    preds_ensemble = (preds_catboost + preds_xgb) / 2\n",
    "else:\n",
    "    preds_ensemble = preds_xgb\n",
    "    \n",
    "ql_ensemble = quantile_loss(y_val.values, preds_ensemble)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    print(f\"\\nCatBoost GPU:\")\n",
    "    print(f\"  Quantile Loss: {ql_catboost:,.0f}\")\n",
    "    print(f\"  Mean prediction: {preds_catboost.mean():,.0f} kg\")\n",
    "    print(f\"  Under-predictions: {np.sum(preds_catboost < y_val.values)/len(y_val):.1%}\")\n",
    "\n",
    "print(f\"\\nXGBoost GPU:\")\n",
    "print(f\"  Quantile Loss: {ql_xgb:,.0f}\")\n",
    "print(f\"  Mean prediction: {preds_xgb.mean():,.0f} kg\")\n",
    "print(f\"  Under-predictions: {np.sum(preds_xgb < y_val.values)/len(y_val):.1%}\")\n",
    "\n",
    "print(f\"\\nEnsemble (avg):\")\n",
    "print(f\"  Quantile Loss: {ql_ensemble:,.0f}\")\n",
    "print(f\"  Mean prediction: {preds_ensemble.mean():,.0f} kg\")\n",
    "print(f\"  Under-predictions: {np.sum(preds_ensemble < y_val.values)/len(y_val):.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Target: 75-80% under-predictions for quantile 0.2\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255e29c",
   "metadata": {},
   "source": [
    "## üîÆ Generate Competition Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb37974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for competition\n",
    "pred_start_date = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "prediction_mapping['horizon_days'] = (prediction_mapping['forecast_end_date'] - pd.to_datetime('2025-01-01')).dt.days\n",
    "\n",
    "predictions_catboost_list = []\n",
    "predictions_xgb_list = []\n",
    "\n",
    "print(\"Generating GPU predictions...\")\n",
    "print(\"This will take 5-10 minutes...\\n\")\n",
    "\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    horizon_days = row['horizon_days']\n",
    "    \n",
    "    features = calculate_advanced_features_v2(\n",
    "        receivals, purchase_orders, materials,\n",
    "        rm_id, pred_start_date, horizon_days\n",
    "    )\n",
    "    \n",
    "    X_pred = pd.DataFrame([features])[feature_cols].fillna(0)\n",
    "    \n",
    "    if CATBOOST_AVAILABLE:\n",
    "        pred_cat = model_catboost.predict(X_pred)[0]\n",
    "        predictions_catboost_list.append(max(0, pred_cat))\n",
    "    \n",
    "    pred_xgb = model_xgb.predict(xgb.DMatrix(X_pred))[0]\n",
    "    predictions_xgb_list.append(max(0, pred_xgb))\n",
    "    \n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"  {idx + 1:,}/30,450\")\n",
    "\n",
    "predictions_xgb_arr = np.array(predictions_xgb_list)\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    predictions_catboost_arr = np.array(predictions_catboost_list)\n",
    "    predictions_ensemble_arr = (predictions_catboost_arr + predictions_xgb_arr) / 2\n",
    "else:\n",
    "    predictions_ensemble_arr = predictions_xgb_arr\n",
    "\n",
    "print(\"\\n‚úÖ Predictions generated!\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    print(f\"\\nCatBoost GPU:\")\n",
    "    print(f\"  Mean: {predictions_catboost_arr.mean():,.0f} kg\")\n",
    "    print(f\"  Median: {np.median(predictions_catboost_arr):,.0f} kg\")\n",
    "    print(f\"  Zeros: {np.sum(predictions_catboost_arr == 0)/len(predictions_catboost_arr):.1%}\")\n",
    "\n",
    "print(f\"\\nXGBoost GPU:\")\n",
    "print(f\"  Mean: {predictions_xgb_arr.mean():,.0f} kg\")\n",
    "print(f\"  Median: {np.median(predictions_xgb_arr):,.0f} kg\")\n",
    "print(f\"  Zeros: {np.sum(predictions_xgb_arr == 0)/len(predictions_xgb_arr):.1%}\")\n",
    "\n",
    "print(f\"\\nEnsemble:\")\n",
    "print(f\"  Mean: {predictions_ensemble_arr.mean():,.0f} kg\")\n",
    "print(f\"  Median: {np.median(predictions_ensemble_arr):,.0f} kg\")\n",
    "print(f\"  Zeros: {np.sum(predictions_ensemble_arr == 0)/len(predictions_ensemble_arr):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1cb65",
   "metadata": {},
   "source": [
    "## üíæ Save Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ecfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions\n",
    "if CATBOOST_AVAILABLE:\n",
    "    submission_catboost = pd.DataFrame({\n",
    "        'ID': range(1, len(predictions_catboost_arr) + 1),\n",
    "        'predicted_weight': predictions_catboost_arr\n",
    "    })\n",
    "    submission_catboost.to_csv('submission_gpu_catboost.csv', index=False)\n",
    "    print(\"‚úÖ Saved: submission_gpu_catboost.csv\")\n",
    "\n",
    "submission_xgb = pd.DataFrame({\n",
    "    'ID': range(1, len(predictions_xgb_arr) + 1),\n",
    "    'predicted_weight': predictions_xgb_arr\n",
    "})\n",
    "submission_xgb.to_csv('submission_gpu_xgboost.csv', index=False)\n",
    "print(\"‚úÖ Saved: submission_gpu_xgboost.csv\")\n",
    "\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    'ID': range(1, len(predictions_ensemble_arr) + 1),\n",
    "    'predicted_weight': predictions_ensemble_arr\n",
    "})\n",
    "submission_ensemble.to_csv('submission_gpu_ensemble.csv', index=False)\n",
    "print(\"‚úÖ Saved: submission_gpu_ensemble.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nV4 (LightGBM tuned): mean ~67,000 kg ‚Üí Score: 13,978 (43rd)\")\n",
    "print(f\"GPU Ensemble: mean ~{predictions_ensemble_arr.mean():,.0f} kg ‚Üí Score: ???\")\n",
    "print(f\"\\nExpected improvement: 20-30%\")\n",
    "print(f\"Target score: ~9,000-11,000\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ RECOMMENDATIONS:\")\n",
    "print(\"  1. Try submission_gpu_ensemble.csv first\")\n",
    "print(\"  2. If too high, try submission_gpu_catboost.csv\")\n",
    "print(\"  3. If still too high, scale down by 0.8x\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
